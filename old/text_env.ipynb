{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translate</th>\n",
       "      <th>source</th>\n",
       "      <th>attr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Ve.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vete.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Vaya.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Go.</td>\n",
       "      <td>Váyase.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hola.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  translate   source                                               attr\n",
       "0       Go.      Ve.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "1       Go.    Vete.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "2       Go.    Vaya.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "3       Go.  Váyase.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
       "4       Hi.    Hola.  CC-BY 2.0 (France) Attribution: tatoeba.org #5..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/spa-eng.zip\", delimiter=\"\\t\", names=[\"translate\", \"source\", \"attr\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ve.', 'Vete.', 'Vaya.', ...,\n",
       "       'Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.',\n",
       "       'Puede que sea imposible obtener un corpus completamente libre de errores debido a la naturaleza de este tipo de esfuerzo de colaboración. Sin embargo, si animamos a los miembros a contribuir frases en sus propios idiomas en lugar de experimentar con los idiomas que están aprendiendo, podríamos ser capaces de minimizar los errores.',\n",
       "       'Un día, me desperté y vi que Dios me había puesto pelo en la cara. Me lo afeité. Al día siguiente, vi que Dios me lo había vuelto a poner en la cara, así que me lo afeité otra vez. Al tercer día, cuando vi que Dios me había puesto pelo en la cara de nuevo, decidí que Dios se saliera con la suya. Por eso tengo barba.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_sent = np.array(df[\"source\"])\n",
    "translate_sent = np.array(df[\"translate\"])\n",
    "source_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-23 04:56:19.285802: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras import layers\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `TextVectorization` layer to vectorize sentences from the corpus. Learn more about using this layer in this [Text classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial. Notice from the first few sentences above that the text needs to be in one case and punctuation needs to be removed. To do this, define a `custom_standardization function` that can be used in the TextVectorization layer.\n",
    "\n",
    "```\n",
    "# Now, create a custom standardization function to lowercase the text and\n",
    "# remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  text = tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation + \"¿¡\"), '')\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text\n",
    "max_features = 5000\n",
    "# sequence_length = 250\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    # output_sequence_length=sequence_length,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = tf.data.Dataset.from_tensor_slices((translate_sent))\n",
    "vectorize_layer.adapt(train_text.batch(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '[End]', '[Begin]', 'i', 'the', 'to', 'you', 'tom', 'a', 'is', 'he', 'in', 'that', 'of', 'it', 'do', 'was', 'me', 'this']\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'We know.'\n",
      "[ 3 27 39  2  0]\n",
      "[Begin] we know [End] \n",
      "\n",
      "b'I sat up.'\n",
      "[  3   4 553  62   2]\n",
      "[Begin] i sat up [End]\n",
      "\n",
      "b'Try this.'\n",
      "[  3 215  19   2   0]\n",
      "[Begin] try this [End] \n",
      "\n",
      "b\"It's hers.\"\n",
      "[   3   44 2310    2    0]\n",
      "[Begin] its hers [End] \n",
      "\n",
      "b\"Don't lie.\"\n",
      "[  3  22 615   2   0]\n",
      "[Begin] dont lie [End] \n",
      "\n",
      "b'Tell me.'\n",
      "[ 3 92 18  2  0]\n",
      "[Begin] tell me [End] \n",
      "\n",
      "b'See above.'\n",
      "[   3   78 1369    2    0]\n",
      "[Begin] see above [End] \n",
      "\n",
      "b'I fled.'\n",
      "[   3    4 3795    2    0]\n",
      "[Begin] i fled [End] \n",
      "\n",
      "b'Try this.'\n",
      "[  3 215  19   2   0]\n",
      "[Begin] try this [End] \n",
      "\n",
      "b'Perfect!'\n",
      "[  3 901   2   0   0]\n",
      "[Begin] perfect [End]  \n",
      "\n",
      "b\"I'm okay.\"\n",
      "[   3   29 2237    2    0]\n",
      "[Begin] im okay [End] \n",
      "\n",
      "b'Go away!'\n",
      "[  3  42 229   2   0]\n",
      "[Begin] go away [End] \n",
      "\n",
      "b'No way!'\n",
      "[  3  71 176   2   0]\n",
      "[Begin] no way [End] \n",
      "\n",
      "b\"I'm calm.\"\n",
      "[   3   29 1056    2    0]\n",
      "[Begin] im calm [End] \n",
      "\n",
      "b'You lost.'\n",
      "[  3   7 222   2   0]\n",
      "[Begin] you lost [End] \n",
      "\n",
      "b'I will go.'\n",
      "[ 3  4 56 42  2]\n",
      "[Begin] i will go [End]\n",
      "\n",
      "b'I am good.'\n",
      "[  3   4 128  77   2]\n",
      "[Begin] i am good [End]\n",
      "\n",
      "b'Put it on.'\n",
      "[  3 152  15  33   2]\n",
      "[Begin] put it on [End]\n",
      "\n",
      "b'Leave now.'\n",
      "[  3 174  90   2   0]\n",
      "[Begin] leave now [End] \n",
      "\n",
      "b'Wait.'\n",
      "[  3 249   2   0   0]\n",
      "[Begin] wait [End]  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "L = 20\n",
    "sentences = next(iter(train_text.shuffle(1000).batch(L)))\n",
    "encoded_sentences = vectorize_layer(sentences)\n",
    "# print(sentences, encoded_sentences)\n",
    "for sen, enc in zip(sentences, encoded_sentences):\n",
    "    print(sen.numpy())\n",
    "    print(enc.numpy())\n",
    "    print(*[vectorize_layer.get_vocabulary()[i] for i in enc.numpy()])\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обратном направлении:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 16\n",
    "embedding = tf.keras.layers.Embedding(max_features+1, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 5, 16), dtype=float32, numpy=\n",
       "array([[[-1.6608346e-02, -2.2104634e-02,  2.3478720e-02, ...,\n",
       "          4.6716955e-02, -2.8066708e-02, -2.5480783e-02],\n",
       "        [-1.8573701e-02,  7.6920390e-03, -1.0748878e-03, ...,\n",
       "         -3.9673388e-02, -2.2727538e-02,  4.8033502e-02],\n",
       "        [-3.5997104e-02,  1.8770646e-02,  1.8370401e-02, ...,\n",
       "          1.9790221e-02, -2.3820806e-02, -3.0015349e-02],\n",
       "        [-3.8259268e-02,  3.0608263e-02, -4.5956898e-02, ...,\n",
       "          4.7998179e-02, -4.0138554e-02, -2.4500713e-03],\n",
       "        [-1.9914960e-02, -1.0289848e-02,  3.0805472e-02, ...,\n",
       "         -4.1862488e-02,  4.8731674e-02,  1.1810042e-02]],\n",
       "\n",
       "       [[-1.6608346e-02, -2.2104634e-02,  2.3478720e-02, ...,\n",
       "          4.6716955e-02, -2.8066708e-02, -2.5480783e-02],\n",
       "        [ 4.6817865e-02,  2.6325714e-02,  1.6582992e-02, ...,\n",
       "          1.0951161e-02,  4.5461629e-02,  2.2797074e-02],\n",
       "        [ 3.4739424e-02,  9.4610676e-03, -1.1408735e-02, ...,\n",
       "          3.0680131e-02,  3.4243714e-02, -4.2525329e-02],\n",
       "        [-2.6112247e-02,  6.6179410e-03,  7.1143731e-03, ...,\n",
       "         -1.0503948e-02,  5.6564808e-05, -9.2624314e-03],\n",
       "        [-3.8259268e-02,  3.0608263e-02, -4.5956898e-02, ...,\n",
       "          4.7998179e-02, -4.0138554e-02, -2.4500713e-03]],\n",
       "\n",
       "       [[-1.6608346e-02, -2.2104634e-02,  2.3478720e-02, ...,\n",
       "          4.6716955e-02, -2.8066708e-02, -2.5480783e-02],\n",
       "        [ 4.4245008e-02, -9.6965805e-03, -2.3333097e-02, ...,\n",
       "         -9.4552264e-03, -2.9254531e-02,  3.4169581e-02],\n",
       "        [ 4.7882330e-02, -4.0008236e-02, -4.5220114e-02, ...,\n",
       "         -2.3216451e-02,  1.2738574e-02,  1.4093187e-02],\n",
       "        [-3.8259268e-02,  3.0608263e-02, -4.5956898e-02, ...,\n",
       "          4.7998179e-02, -4.0138554e-02, -2.4500713e-03],\n",
       "        [-1.9914960e-02, -1.0289848e-02,  3.0805472e-02, ...,\n",
       "         -4.1862488e-02,  4.8731674e-02,  1.1810042e-02]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.6608346e-02, -2.2104634e-02,  2.3478720e-02, ...,\n",
       "          4.6716955e-02, -2.8066708e-02, -2.5480783e-02],\n",
       "        [-6.3849799e-03, -3.2054413e-02,  8.2895160e-03, ...,\n",
       "          2.2017431e-02,  3.6419298e-02, -4.0725339e-02],\n",
       "        [-1.2595296e-02,  1.5260074e-02,  4.8706617e-02, ...,\n",
       "         -3.2493554e-02,  1.3161149e-02, -4.3726336e-02],\n",
       "        [ 2.8057385e-02, -2.9036868e-02,  3.2959942e-02, ...,\n",
       "          4.3712854e-03, -4.2822134e-02,  2.1269988e-02],\n",
       "        [-3.8259268e-02,  3.0608263e-02, -4.5956898e-02, ...,\n",
       "          4.7998179e-02, -4.0138554e-02, -2.4500713e-03]],\n",
       "\n",
       "       [[-1.6608346e-02, -2.2104634e-02,  2.3478720e-02, ...,\n",
       "          4.6716955e-02, -2.8066708e-02, -2.5480783e-02],\n",
       "        [ 3.8810682e-02, -1.4578890e-02, -1.3836205e-02, ...,\n",
       "          2.6916590e-02,  3.6320221e-02,  5.5669062e-03],\n",
       "        [-3.0383324e-02, -1.1044908e-02, -3.9490152e-02, ...,\n",
       "          3.3803057e-02, -3.6295511e-02,  3.9320920e-02],\n",
       "        [-3.8259268e-02,  3.0608263e-02, -4.5956898e-02, ...,\n",
       "          4.7998179e-02, -4.0138554e-02, -2.4500713e-03],\n",
       "        [-1.9914960e-02, -1.0289848e-02,  3.0805472e-02, ...,\n",
       "         -4.1862488e-02,  4.8731674e-02,  1.1810042e-02]],\n",
       "\n",
       "       [[-1.6608346e-02, -2.2104634e-02,  2.3478720e-02, ...,\n",
       "          4.6716955e-02, -2.8066708e-02, -2.5480783e-02],\n",
       "        [-4.5940887e-02,  3.5082307e-02, -1.1043452e-02, ...,\n",
       "          2.2216607e-02, -1.3332881e-02, -4.8769750e-02],\n",
       "        [-3.8259268e-02,  3.0608263e-02, -4.5956898e-02, ...,\n",
       "          4.7998179e-02, -4.0138554e-02, -2.4500713e-03],\n",
       "        [-1.9914960e-02, -1.0289848e-02,  3.0805472e-02, ...,\n",
       "         -4.1862488e-02,  4.8731674e-02,  1.1810042e-02],\n",
       "        [-1.9914960e-02, -1.0289848e-02,  3.0805472e-02, ...,\n",
       "         -4.1862488e-02,  4.8731674e-02,  1.1810042e-02]]], dtype=float32)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(encoded_sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно еще какие-то ragged тензоры использовать вместо padding-а."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', '[End]', '[Begin]', 'i', 'the', 'to', 'you', 'tom', 'a', 'is', 'he', 'in', 'that', 'of', 'it', 'do', 'was', 'me', 'this']\n",
      "b'Try hard.'\n",
      "[  3 215 247   2]\n",
      "[Begin] try hard [End]\n",
      "\n",
      "b'I hit Tom.'\n",
      "[  3   4 535   8   2]\n",
      "[Begin] i hit tom [End]\n",
      "\n",
      "b'Is he Tom?'\n",
      "[ 3 10 11  8  2]\n",
      "[Begin] is he tom [End]\n",
      "\n",
      "b'Get real!'\n",
      "[  3  63 665   2]\n",
      "[Begin] get real [End]\n",
      "\n",
      "b'Who am I?'\n",
      "[  3  82 128   4   2]\n",
      "[Begin] who am i [End]\n",
      "\n",
      "b'Keep this.'\n",
      "[  3 220  19   2]\n",
      "[Begin] keep this [End]\n",
      "\n",
      "b'Watch me.'\n",
      "[  3 310  18   2]\n",
      "[Begin] watch me [End]\n",
      "\n",
      "b\"I'll stop.\"\n",
      "[  3  76 204   2]\n",
      "[Begin] ill stop [End]\n",
      "\n",
      "b'Wait up.'\n",
      "[  3 249  62   2]\n",
      "[Begin] wait up [End]\n",
      "\n",
      "b'Am I fat?'\n",
      "[   3  128    4 1013    2]\n",
      "[Begin] am i fat [End]\n",
      "\n",
      "b'Be fair.'\n",
      "[   3   31 1423    2]\n",
      "[Begin] be fair [End]\n",
      "\n",
      "b'Sign this.'\n",
      "[  3 944  19   2]\n",
      "[Begin] sign this [End]\n",
      "\n",
      "b'Hi, guys.'\n",
      "[   3 1904  763    2]\n",
      "[Begin] hi guys [End]\n",
      "\n",
      "b'Hit Tom.'\n",
      "[  3 535   8   2]\n",
      "[Begin] hit tom [End]\n",
      "\n",
      "b'I quit.'\n",
      "[  3   4 666   2]\n",
      "[Begin] i quit [End]\n",
      "\n",
      "b\"Don't die.\"\n",
      "[  3  22 539   2]\n",
      "[Begin] dont die [End]\n",
      "\n",
      "b'Seize him!'\n",
      "[   3 3350   43    2]\n",
      "[Begin] seize him [End]\n",
      "\n",
      "b'Let me go!'\n",
      "[  3 158  18  42   2]\n",
      "[Begin] let me go [End]\n",
      "\n",
      "b'I gave up.'\n",
      "[  3   4 232  62   2]\n",
      "[Begin] i gave up [End]\n",
      "\n",
      "b'Wait!'\n",
      "[  3 249   2]\n",
      "[Begin] wait [End]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorize_layer_rag = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    # output_sequence_length=sequence_length,\n",
    "    ragged=True\n",
    "    )\n",
    "\n",
    "train_text = tf.data.Dataset.from_tensor_slices((translate_sent))\n",
    "vectorize_layer_rag.adapt(train_text.batch(100))\n",
    "inverse_vocab = vectorize_layer_rag.get_vocabulary()\n",
    "print(inverse_vocab[:20])\n",
    "L = 20\n",
    "sentences = next(iter(train_text.shuffle(1000).batch(L)))\n",
    "encoded_sentences = vectorize_layer_rag(sentences)\n",
    "# print(sentences, encoded_sentences)\n",
    "for sen, enc in zip(sentences, encoded_sentences):\n",
    "    print(sen.numpy())\n",
    "    print(enc.numpy())\n",
    "    print(*[vectorize_layer_rag.get_vocabulary()[i] for i in enc.numpy()])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(20, 5), dtype=int64, numpy=\n",
       " array([[   3,  215,  247,    2,    0],\n",
       "        [   3,    4,  535,    8,    2],\n",
       "        [   3,   10,   11,    8,    2],\n",
       "        [   3,   63,  665,    2,    0],\n",
       "        [   3,   82,  128,    4,    2],\n",
       "        [   3,  220,   19,    2,    0],\n",
       "        [   3,  310,   18,    2,    0],\n",
       "        [   3,   76,  204,    2,    0],\n",
       "        [   3,  249,   62,    2,    0],\n",
       "        [   3,  128,    4, 1013,    2],\n",
       "        [   3,   31, 1423,    2,    0],\n",
       "        [   3,  944,   19,    2,    0],\n",
       "        [   3, 1904,  763,    2,    0],\n",
       "        [   3,  535,    8,    2,    0],\n",
       "        [   3,    4,  666,    2,    0],\n",
       "        [   3,   22,  539,    2,    0],\n",
       "        [   3, 3350,   43,    2,    0],\n",
       "        [   3,  158,   18,   42,    2],\n",
       "        [   3,    4,  232,   62,    2],\n",
       "        [   3,  249,    2,    0,    0]])>,\n",
       " <tf.RaggedTensor [[3, 215, 247, 2], [3, 4, 535, 8, 2], [3, 10, 11, 8, 2], [3, 63, 665, 2],\n",
       "  [3, 82, 128, 4, 2], [3, 220, 19, 2], [3, 310, 18, 2], [3, 76, 204, 2],\n",
       "  [3, 249, 62, 2], [3, 128, 4, 1013, 2], [3, 31, 1423, 2], [3, 944, 19, 2],\n",
       "  [3, 1904, 763, 2], [3, 535, 8, 2], [3, 4, 666, 2], [3, 22, 539, 2],\n",
       "  [3, 3350, 43, 2], [3, 158, 18, 42, 2], [3, 4, 232, 62, 2], [3, 249, 2]]>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer(sentences), vectorize_layer_rag(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 5), dtype=int64, numpy=\n",
       "array([[   3,  215,  247,    2,    0],\n",
       "       [   3,    4,  535,    8,    2],\n",
       "       [   3,   10,   11,    8,    2],\n",
       "       [   3,   63,  665,    2,    0],\n",
       "       [   3,   82,  128,    4,    2],\n",
       "       [   3,  220,   19,    2,    0],\n",
       "       [   3,  310,   18,    2,    0],\n",
       "       [   3,   76,  204,    2,    0],\n",
       "       [   3,  249,   62,    2,    0],\n",
       "       [   3,  128,    4, 1013,    2],\n",
       "       [   3,   31, 1423,    2,    0],\n",
       "       [   3,  944,   19,    2,    0],\n",
       "       [   3, 1904,  763,    2,    0],\n",
       "       [   3,  535,    8,    2,    0],\n",
       "       [   3,    4,  666,    2,    0],\n",
       "       [   3,   22,  539,    2,    0],\n",
       "       [   3, 3350,   43,    2,    0],\n",
       "       [   3,  158,   18,   42,    2],\n",
       "       [   3,    4,  232,   62,    2],\n",
       "       [   3,  249,    2,    0,    0]])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# и этом может быть легко конвертированно в обычный тензор с помощью .to_tensor()\n",
    "vectorize_layer_rag(sentences).to_tensor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разбираюсь, как пользоваться рекуррентными сетями в tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 32), dtype=tf.float32, name=None), name='rnn_2/strided_slice_3:0', description=\"created by layer 'rnn_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 64), dtype=tf.float32, name=None), name='rnn_3/strided_slice_3:0', description=\"created by layer 'rnn_3'\")\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import RNN\n",
    "from keras import backend\n",
    "\n",
    "# First, let's define a RNN Cell, as a layer subclass.\n",
    "class MinimalRNNCell(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units, **kwargs):\n",
    "        self.units = units\n",
    "        self.state_size = units\n",
    "        super(MinimalRNNCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                      initializer='uniform',\n",
    "                                      name='kernel')\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units),\n",
    "            initializer='uniform',\n",
    "            name='recurrent_kernel')\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        prev_output = states[0]\n",
    "        h = backend.dot(inputs, self.kernel)\n",
    "        output = h + backend.dot(prev_output, self.recurrent_kernel)\n",
    "        return output, [output]\n",
    "\n",
    "# Let's use this cell in a RNN layer:\n",
    "\n",
    "cell = MinimalRNNCell(32)\n",
    "x = keras.Input((None, 5))\n",
    "layer = RNN(cell)\n",
    "y = layer(x)\n",
    "print(y)\n",
    "# Here's how to use the cell to build a stacked RNN:\n",
    "\n",
    "cells = [MinimalRNNCell(32), MinimalRNNCell(64)]\n",
    "x = keras.Input((None, 5))\n",
    "layer = RNN(cells)\n",
    "y = layer(x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reproduce translate tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CRNN-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
